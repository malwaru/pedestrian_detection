{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbebf585",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Essential imports\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import joblib\n",
    "import cv2\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import copy \n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import LinearSVC\n",
    "from skimage.transform import pyramid_gaussian\n",
    "from imutils.object_detection import non_max_suppression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99548a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SvmTrainer():\n",
    "    def __init__(self):\n",
    "        ## Default values\n",
    "        print(f\"Staring the Classifier\")\n",
    "        self.train_data=[]\n",
    "        self.train_labels=[]\n",
    "        self.classifier=LinearSVC()\n",
    "    \n",
    "    def load_train_data(self,file_path,data_type=\"*.png\",data_class=1):\n",
    "        '''\n",
    "        Load the images from the filepath and get the HoG featues\n",
    "        If dataclass=1 positive images \n",
    "        If dataclass=0 negative images\n",
    "        '''\n",
    "        files_found=False\n",
    "        for filename in glob.glob(os.path.join(file_path,data_type)):\n",
    "            files_found=True\n",
    "            current_image=cv2.imread(filename)\n",
    "            current_image=cv2.resize(current_image,(64,128))\n",
    "            current_image=cv2.cvtColor(current_image,cv2.COLOR_BGR2GRAY)\n",
    "            hog_features=hog(image=current_image,orientations=9,pixels_per_cell=(8,8),visualize=False,cells_per_block=(3,3))\n",
    "            self.train_data.append(hog_features)\n",
    "            self.train_labels.append(data_class)\n",
    "            \n",
    "        if files_found:\n",
    "            print(f\" HoG Feature extractor complete\")\n",
    "        else:\n",
    "            raise Exception(\"No such file or folder\")\n",
    "            \n",
    "    def train_svm(self,model_save_path=\"./\"):\n",
    "        '''\n",
    "        This function calls for training the data set\n",
    "        '''\n",
    "        ## Asserts if there are training data\n",
    "        assert len(self.train_data)>0,f\"No training data available\"\n",
    "        # Convert to numpy arrays\n",
    "        self.train_data = np.float32(self.train_data)\n",
    "        self.train_labels= np.array(self.train_labels)\n",
    "        #Fitting the data\n",
    "        self.classifier.fit(self.train_data,self.train_labels)\n",
    "        print(f\"Training complete saving model at {model_save_path}\")\n",
    "        now = datetime.now()\n",
    "        # dd/mm/YY H:M:S\n",
    "        dt_string = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "        model_name=model_save_path+\"_svm_model\"+dt_string+\".dat\"\n",
    "        joblib.dump(self.classifier,model_name)\n",
    "        print(f\"Model saved as at {model_name}\")\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c5d32a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PersonDetecor():\n",
    "    def __init__(self,path_test: str,path_anno: str,model_path:str)-> None:\n",
    "        '''\n",
    "        Initiate the person detector class\n",
    "        \n",
    "        Params\n",
    "        ----------\n",
    "        path_test : Relative file path with the test images\n",
    "        path_anno : Relative path with the annotations \n",
    "        '''\n",
    "        self.path_test=path_test\n",
    "        self.path_anno=path_anno\n",
    "        self.downscale_factor=1.5\n",
    "        self.classifier=joblib.load(model_path)\n",
    "        # Size of the detections used in training \n",
    "        self.detect_size=(64,128)\n",
    "        self.step_size=(10,10)\n",
    "        ## The confusion matrix\n",
    "        ## TP,FP | FN , TN\n",
    "        self.CM=[[0,0],[0,0]]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def nms(self):\n",
    "        '''\n",
    "        Fast Non-maximum-supression method based on Malisiewicz et al \n",
    "                    \n",
    "        \n",
    "        \n",
    "        References:\n",
    "        The https://github.com/quantombone/exemplarsvm\n",
    "        https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/\n",
    "        https://pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n",
    "        https://nms.readthedocs.io/en/latest/_modules/nms/malisiewicz.html\n",
    "        '''\n",
    "        \n",
    "        raise NotImplementedError(\"Please implement the method\")\n",
    "        \n",
    "    def metrics(self,detections,ground_truths):\n",
    "        '''\n",
    "        Calculate the performance metrics of the method \n",
    "        \n",
    "        Measure following metrics         \n",
    "            meanAveragePrecision\n",
    "            average IoU\n",
    "            \n",
    "            \n",
    "        References:\n",
    "        https://towardsdatascience.com/on-object-detection-metrics-with-worked-example-216f173ed31e\n",
    "        https://www.v7labs.com/blog/mean-average-precision\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        ##IOU calculation\n",
    "        for detection in detections:            \n",
    "            x_mid=detection[0]+detection[2]/2\n",
    "            y_mid=detection[1]+detection[3]/2\n",
    "        for gt in ground_truths:\n",
    "            gt=0\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        raise NotImplementedError(\"Implement me\")\n",
    "        \n",
    "    def sliding_window(self,image,window_size,step_size):\n",
    "        ''''\n",
    "        Sliding window approach to pass throught the image\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        window_size: Tuple (2x2)\n",
    "                    The size of the window\n",
    "        \n",
    "        step_size : \n",
    "                            \n",
    "        \n",
    "                        the size of the window that wil p \n",
    "        Returns\n",
    "        -------\n",
    "        x : ndarray, shape Q\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        for y in range(0,image.shape[0],step_size[1]):\n",
    "            for x in range(0,image.shape[1],step_size[0]):\n",
    "                yield (y,x,image[x:window_size[0],y:window_size[1]])       \n",
    "        \n",
    "        \n",
    "    def visualise(self,image,bBox,confidence):\n",
    "        '''\n",
    "        Visualise the result bBox on image\n",
    "        '''\n",
    "        \n",
    "        figure1=plt.figure(figsize=(25,10))\n",
    "        ax1=figure1.add_subplot(111)\n",
    "        cv2.rectangle(image,(bBox[0],bBox[2]),(bBox[1],bBox[3]),(0,0,255),2)\n",
    "        cv2.putText(image,\"Confidence: \"+str(confidence),(bBox[0],bBox[2]-10))\n",
    "        ax1.imshow(image)\n",
    "        ax1.set(title=\"Prediction\",xlabel=\"X\",ylabel=\"Y\")     \n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def detect_persons_multi(self,data_type: str,metrics:bool=False,visualise:bool=False)-> None:\n",
    "        '''\n",
    "        Detect people in multiple images in a given folder \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        data_type: string\n",
    "                    the data type of images eg: png,jpeg\n",
    "        metrics  : bool\n",
    "                    Boolean to calculate metrics\n",
    "        visualise : bool\n",
    "                    Visualise the results\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        for filename in glob.glob(os.path.join(self.path_test,data_type)):\n",
    "            current_detections=[]\n",
    "            current_scale=0\n",
    "            current_image=cv2.imread(filename)\n",
    "            ## Maybe resize the image\n",
    "            \n",
    "            for scaled_image in pyramid_gaussian(current_image):\n",
    "                if scaled_image.shape[0]<self.detect_size[0] or scaled_image.shape[1]<self.detect_size[1]:\n",
    "                    #Loop breaks when the the scale reaches smaller than the images\n",
    "                    # that was used to train the classifier\n",
    "                    break\n",
    "                for (x,y,cropped_image) in sliding_window(scaled_image,self.detect_size,self.step_size):\n",
    "                    #Convert to Grayscale for preprocessing\n",
    "                    cropped_image=cv2.cvtColor(cropped_image,cv2.COLOR_BGR2GRAY)\n",
    "                    #Create HoG features\n",
    "                    hog_feature=hog(cropped_image,\n",
    "                                orientations=9,\n",
    "                                pixels_per_cell=(8, 8),\n",
    "                                cells_per_block=(3, 3),\n",
    "                                block_norm='L2-Hys',\n",
    "                                visualize=False,\n",
    "                                transform_sqrt=False,\n",
    "                                feature_vector=True,\n",
    "                                multichannel=None)\n",
    "                    # Flatten the features\n",
    "                    hog_feature=hog_feature.reshape(1,-1)\n",
    "                    # Pass it through the trained model\n",
    "                    prediction=self.classifier(hog_feature)\n",
    "                    current_confidence=self.classifier.decision_function(hog_feature)\n",
    "                    \n",
    "                    ##If person is detected with a confidence over 50%\n",
    "                    if (prediction==1) and (current_confidence>0.5):\n",
    "                            x=int(x * (self.downscale_factor**current_scale))\n",
    "                            y=int(y* (self.downscale_factor**current_scale))\n",
    "                            w=int(self.detect_size[0]*(self.downscale_factor**current_scale))\n",
    "                            h=int(self.detect_size[1]*(self.downscale_factor**current_scale))\n",
    "                            current_detections.append(x,y,w,h,current_confidence)\n",
    "                current_scale+=1\n",
    "            #End of current image detections\n",
    "            # If there are detections look work on the non max supression \n",
    "            if len(current_detections)>0:                \n",
    "                bBoxs=np.array([[x,y,x+w,y+h] for (x,y,w,h,_) in current_detections])\n",
    "                confidences=[conf for (_,_,_,_,conf) in current_detections]\n",
    "                max_box=non_max_suppression(bBoxs,probs=confidences,overlapThresh=0.3)\n",
    "\n",
    "                \n",
    "    def detect_persons_single(self,file_name: str,metrics:bool=False,visualise:bool=False)-> None:\n",
    "        '''\n",
    "        Detect people in a single image\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        data_type: string\n",
    "                    the data type of images eg: png,jpeg\n",
    "        metrics  : bool\n",
    "                    Boolean to calculate metrics\n",
    "        visualise : bool\n",
    "                    Visualise the results\n",
    "\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "        filename =os.path.join(self.path_test,file_name)\n",
    "        current_detections=[]\n",
    "        current_scale=0\n",
    "        current_image=cv2.imread(filename)\n",
    "        ## Maybe resize the image\n",
    "\n",
    "        for scaled_image in pyramid_gaussian(current_image):\n",
    "            if scaled_image.shape[0]<self.detect_size[0] or scaled_image.shape[1]<self.detect_size[1]:\n",
    "                #Loop breaks when the the scale reaches smaller than the images\n",
    "                # that was used to train the classifier\n",
    "                break\n",
    "            for (x,y,cropped_image) in self.sliding_window(scaled_image,self.detect_size,self.step_size):  \n",
    "                #Change to format that opencv can handle\n",
    "                # cropped_image.astype(np.uint8)\n",
    "                # figure1=plt.figure(figsize=(25,10))\n",
    "                # ax1=figure1.add_subplot(111)\n",
    "                # ax1.imshow(cropped_image)\n",
    "                # ax1.set(title=\"Prediction\",xlabel=\"X\",ylabel=\"Y\")     \n",
    "                # plt.show()\n",
    "                # #Convert to Grayscale for preprocessing\n",
    "                # cropped_image=cv2.cvtColor(cropped_image,cv2.COLOR_BGR2GRAY)\n",
    "                #Create HoG features\n",
    "                hog_feature=hog(cropped_image,\n",
    "                            orientations=9,\n",
    "                            pixels_per_cell=(8, 8),\n",
    "                            cells_per_block=(3, 3))\n",
    "                # Flatten the features\n",
    "                hog_feature=hog_feature.reshape(1,-1)\n",
    "                print(f\"hog has features of size {hog_feature.shape}\")\n",
    "                # Pass it through the trained model\n",
    "                prediction=self.classifier.predict(hog_feature)\n",
    "                current_confidence=self.classifier.decision_function(hog_feature)\n",
    "\n",
    "                ##If person is detected with a confidence over 50%\n",
    "                if (prediction==1) and (current_confidence>0.5):\n",
    "                        x=int(x * (self.downscale_factor**current_scale))\n",
    "                        y=int(y* (self.downscale_factor**current_scale))\n",
    "                        w=int(self.detect_size[0]*(self.downscale_factor**current_scale))\n",
    "                        h=int(self.detect_size[1]*(self.downscale_factor**current_scale))\n",
    "                        current_detections.append(x,y,w,h,current_confidence)\n",
    "            current_scale+=1\n",
    "        #End of current image detections\n",
    "        # If there are detections look work on the non max supression \n",
    "        if len(current_detections)>0:                \n",
    "            bBoxs=np.array([[x,y,x+w,y+h] for (x,y,w,h,_) in current_detections])\n",
    "            confidences=[conf for (_,_,_,_,conf) in current_detections]\n",
    "            max_box=non_max_suppression(bBoxs,probs=confidences,overlapThresh=0.3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d27437b-2144-418f-b74d-a8384968fe34",
   "metadata": {},
   "source": [
    "## Testing the trainer\n",
    "trainer=SvmTrainer()\n",
    "##data paths\n",
    "postive_sample_path=\"./data/svm_data/positive\"\n",
    "negative_sample_path=\"./data/svm_data/negative\"\n",
    "##Loading positive data\n",
    "trainer.load_train_data(file_path=postive_sample_path,data_type=\"*.png\",data_class=1)\n",
    "trainer.load_train_data(file_path=negative_sample_path,data_type=\"*.jpg\",data_class=0)\n",
    "trainer.load_train_data(file_path=negative_sample_path,data_type=\"*.png\",data_class=0)\n",
    "\n",
    "trainer.train_svm(model_save_path=\"./data/models/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4ef449-3dba-4d45-9ec8-0ec7e889b8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hog has features of size (1, 6804)\n",
      "hog has features of size (1, 4536)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 4536 features per sample; expecting 6804",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mberlin_000122_000019_leftImg8bit.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m svm_classifier\u001b[38;5;241m=\u001b[39mPersonDetecor(path_test\u001b[38;5;241m=\u001b[39mtest_path,path_anno\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/models/svm_model_28_11_2022_22_54_16.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43msvm_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_persons_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvisualise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mPersonDetecor.detect_persons_single\u001b[0;34m(self, file_name, metrics, visualise)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhog has features of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhog_feature\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Pass it through the trained model\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m prediction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhog_feature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m current_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mdecision_function(hog_feature)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m##If person is detected with a confidence over 50%\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/linear_model/_base.py:293\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    Predict class labels for samples in X.\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m        Predicted class label per sample.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    295\u001b[0m         indices \u001b[38;5;241m=\u001b[39m (scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/linear_model/_base.py:272\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    270\u001b[0m n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m n_features:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features per sample; expecting \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m                      \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], n_features))\n\u001b[1;32m    275\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT,\n\u001b[1;32m    276\u001b[0m                          dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "\u001b[0;31mValueError\u001b[0m: X has 4536 features per sample; expecting 6804"
     ]
    }
   ],
   "source": [
    "\n",
    "test_path=\"./data/rawdata/leftImg8bit/test/berlin\"\n",
    "file_name=\"berlin_000122_000019_leftImg8bit.png\"\n",
    "svm_classifier=PersonDetecor(path_test=test_path,path_anno=\"\",model_path=\"./data/models/svm_model_28_11_2022_22_54_16.dat\")\n",
    "svm_classifier.detect_persons_single(file_name=file_name,visualise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22b063-e77c-4bc2-9e85-108eb2c576a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
